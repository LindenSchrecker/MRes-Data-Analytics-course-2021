{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/FlorianSong/MResAMS_DataAnalytics/blob/main/Workshop2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analytics 2021\n",
    "### Workshop 2 &ndash; Machine Learning Classics: Supervised Learning &ndash; 11th November 2021\n",
    "##### Taught by: Linden Schrecker, Annabel Basford, Sophia Yaliraki\n",
    "##### Written by: Florian Song Linden Schrecker, Nan Wu, Sophia Yaliraki\n",
    "\n",
    "Much of today's workshop was taken from https://github.com/ageron/handson-ml2/ which in turn is based on the second edition of an O'Reilly book [Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) by Aurélien Geron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview & Outline\n",
    "\n",
    "\n",
    "#### Topics covered today:\n",
    "1. Decision Trees\n",
    "    * Predicting classes and class probabilities\n",
    "    * Sensitivity to training set details\n",
    "    * Regularisation\n",
    "    * *Extra Material: Decision Tree Regression*\n",
    "2. Ensemble Learning: Random Forests\n",
    "    * Ensemble Learning\n",
    "    * Bagging & Pasting\n",
    "    * Random Forests\n",
    "    * *Extra Material: Stacking*\n",
    "3. Extra material: Support Vector Machines\n",
    "    * Linear SVM Classification\n",
    "    * Sensitivity to feature scales\n",
    "    * Sensitivity to outliers\n",
    "    * Large margin *vs* margin violations\n",
    "    * Non-linear classification\n",
    "    * The kernel trick\n",
    "    * The Gaussian RBF Kernel for Support Vector Machines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, some house-keeping. We need to check if we have all the necessary libraries installed. For this we can use the following boiler plate code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "\n",
    "# all the functions from sklearn needed for this workshop\n",
    "import sklearn\n",
    "from sklearn.datasets import make_moons, make_circles, load_iris, load_wine\n",
    "from sklearn.svm import LinearSVC, SVC, SVR\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.tree import export_text\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Decision Trees\n",
    "\n",
    "- Versatile Machine Learning algorithms for both classification and regression\n",
    "- Fundamental components of **Random Forests**\n",
    "- Pros: \n",
    "    * require very little data preparation\n",
    "    * make very few assumptions about training data\n",
    "    * very easy to comprehend and visualise\n",
    "    * predictions are very fast\n",
    "- Cons: \n",
    "    * the default algorithm (CART) is greedy, i.e. it produces a solution that’s reasonably good but not guaranteed to be optimal (the optimal solution is intractable even for small datasets)\n",
    "    * Very sensitive to data set variations -- small details lead to large changes in the model\n",
    "\n",
    "Good documentation: https://scikit-learn.org/stable/modules/tree.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=3)\n",
    "tree_clf.fit(X, y)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plot_tree(tree_clf,\n",
    "          feature_names=iris.feature_names[2:],\n",
    "          class_names=iris.target_names,\n",
    "          rounded=True,\n",
    "          filled=True\n",
    ")\n",
    "plt.show() #This line is only here to suppress the plot_tree output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each white box represents a question, which can either be true or false\n",
    "- Each coloured box represents a *leaf node*, which gives the final classification\n",
    "- The `gini` values refer to the Gini impurity  \n",
    "$\n",
    "G_i = 1 - \\sum\\limits_{k=1}^{n}{{p_{i,k}}^2},\n",
    "$  \n",
    "where $p_{i,k}$ is the ratio of class $k$ instances among the training instances in the $i$th node.\n",
    "- A node is \"pure” (`gini=0`) if all training instances it applies to belong to the same class\n",
    "- The depth-2 left node (versicolor) has a `gini` score equal to $1- \\frac{0}{54}^2 - \\frac{49}{54}^2 - \\frac{5}{54}^2 \\approx 0.168$.\n",
    "- Notice how these numbers correspond to `value` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(clf, X, y, axes=[0, 7.5, 0, 3], iris=True, legend=False, plot_training=True, \n",
    "                           labels = (\"Iris setosa\", \"Iris versicolor\", \"Iris virginica\"), \n",
    "                           custom_cmap = ['#fafab0','#a0faa0','#9898ff']) :\n",
    "    x1s = np.linspace(axes[0], axes[1], 500)\n",
    "    x2s = np.linspace(axes[2], axes[3], 500)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap(custom_cmap)\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if not iris:\n",
    "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap, alpha=0.8)\n",
    "    if plot_training:\n",
    "        plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=labels[0])\n",
    "        plt.plot(X[:, 0][y==2], X[:, 1][y==2], \"bs\", label=labels[2])\n",
    "        plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\", label=labels[1])\n",
    "        plt.axis(axes)\n",
    "    if iris:\n",
    "        plt.xlabel(\"Petal length\", fontsize=14)\n",
    "        plt.ylabel(\"Petal width\", fontsize=14)\n",
    "    else:\n",
    "        plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "        plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n",
    "    if legend:\n",
    "        plt.legend(loc=\"lower right\", fontsize=14)\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "plot_decision_boundary(tree_clf, X, y, legend=True)\n",
    "plt.plot([2.45, 2.45], [0, 3], \"k-\", linewidth=2)\n",
    "plt.text(1.40, 1.0, \"Depth=0\", fontsize=15)\n",
    "plt.plot([2.45, 7.5], [1.75, 1.75], \"k--\", linewidth=2)\n",
    "plt.text(3.2, 1.80, \"Depth=1\", fontsize=13)\n",
    "plt.plot([4.95, 4.95], [0, 1.75], \"k:\", linewidth=2)\n",
    "plt.plot([4.85, 4.85], [1.75, 3], \"k:\", linewidth=2)\n",
    "plt.text(4.05, 0.5, \"(Depth=2)\", fontsize=11)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting classes and class probabilities\n",
    "\n",
    "- Classes are predicted by starting at the *root node* at the top of the tree and progressing down the tree by answering the respective questions (e.g. is the petal length smaller than 2.45cm?) \n",
    "- Class probabilities are estimated by returning the ratio of training instances of class $k$ in this node\n",
    "- For example, imagine a flower with petal length = 5cm and petal width = 1.5cm:\n",
    "    * The predicted class is virginica, since petal length is >2.45cm (right route at depth=0), petal width is <1.75cm (left route at depth=1) and finally, petal length >4.95 cm (right route at depth=2).\n",
    "    * The corresponding leaf node is the depth-3 second-from-the-left node (with samples=6), so the Decision Tree should output the following probabilities: 0% for Iris setosa (0/6), 33.33% for Iris versicolor (2/6), and 66.66% for Iris virginica (4/6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf.predict([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity to training set details\n",
    "\n",
    "- Sadly, Decision Trees are very sensitive to small changes in the dataset. \n",
    "- Firstly, removing even one flower (the largest Iris versicolor) changes the Decision Tree immensely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[(X[:, 1]==X[:, 1][y==1].max()) & (y==1)] # widest Iris versicolor flower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "not_widest_versicolor = (X[:, 1]!=1.8) | (y!=1)\n",
    "X_tweaked = X[not_widest_versicolor]\n",
    "y_tweaked = y[not_widest_versicolor]\n",
    "\n",
    "tree_clf_tweaked = DecisionTreeClassifier(max_depth=3, random_state=40)\n",
    "tree_clf_tweaked.fit(X_tweaked, y_tweaked)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "plot_decision_boundary(tree_clf_tweaked, X_tweaked, y_tweaked)\n",
    "plt.plot([0, 7.5], [0.8, 0.8], \"k-\", linewidth=2)\n",
    "plt.text(1.0, 0.9, \"Depth=0\", fontsize=15)\n",
    "plt.plot([0, 7.5], [1.75, 1.75], \"k--\", linewidth=2)\n",
    "plt.text(1.0, 1.80, \"Depth=1\", fontsize=13)\n",
    "plt.plot([4.95, 4.95], [0.8, 1.75], \"k:\", linewidth=2)\n",
    "plt.text(5, 1, \"(Depth=2)\", fontsize=11)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "\n",
    "plot_tree(tree_clf_tweaked,\n",
    "          feature_names=iris.feature_names[2:],\n",
    "          class_names=iris.target_names,\n",
    "          rounded=True,\n",
    "          filled=True\n",
    ")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Furthermore, rotating the data can produce further changes, as decision trees can only produce boundaries along axes:\n",
    "\n",
    "*N.B. for this simple data transformation, we can use a **rotation matrix**, defined like this:*\n",
    "$R = \\begin{pmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{pmatrix} $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = np.pi / 180 * 20 # angle has to be in radians\n",
    "rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\n",
    "Xr = X.dot(rotation_matrix)\n",
    "\n",
    "tree_clf_r = DecisionTreeClassifier(max_depth=3)\n",
    "tree_clf_r.fit(Xr, y)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "plot_decision_boundary(tree_clf_r, Xr, y, axes=[0.5, 7.5, -1.0, 1])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "\n",
    "plot_tree(tree_clf_r,\n",
    "          feature_names=iris.feature_names[2:],\n",
    "          class_names=iris.target_names,\n",
    "          rounded=True,\n",
    "          filled=True\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularisation\n",
    "\n",
    "- To avoid overfitting, can use various parameters to restrict the Decision Tree:\n",
    "    * `max_depth`: the maximum depth\n",
    "    * `min_samples_split`: minimum number of samples a node must have before it can be split\n",
    "    * `min_samples_leaf`: minimum number of samples a leaf node must have\n",
    "    * `min_weight_fraction_leaf`: same as above but expressed as a fraction of the total number of weighted instances\n",
    "    * `max_leaf_nodes`: maximum number of leaf nodes\n",
    "    * `max_features`: maximum number of features that are evaluated for splitting at each node\n",
    "    \n",
    "For this part, we introduce a new dataset. More accurately, this is a dataset *generator*: `make_moons()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xm_small, ym_small = make_moons(n_samples=100, noise=0.01)\n",
    "Xm, ym = make_moons(n_samples=100, noise=0.25)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plt.plot(Xm_small[:, 0][ym_small==0], Xm_small[:, 1][ym_small==0], \"yo\")\n",
    "plt.plot(Xm_small[:, 0][ym_small==1], Xm_small[:, 1][ym_small==1], \"g^\")\n",
    "plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(r\"$x_2$\", rotation=0, fontsize=18)\n",
    "plt.title(\"Moon dataset with minimal noise\")\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.plot(Xm[:, 0][ym==0], Xm[:, 1][ym==0], \"yo\")\n",
    "plt.plot(Xm[:, 0][ym==1], Xm[:, 1][ym==1], \"g^\")\n",
    "plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "plt.title(\"Moon dataset with some noise\")\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xm, ym = make_moons(n_samples=100, noise=0.25, random_state=42)\n",
    "\n",
    "deep_tree_clf1 = DecisionTreeClassifier()\n",
    "deep_tree_clf2 = DecisionTreeClassifier(min_samples_leaf=3) #change this number to see how the regularisation works!\n",
    "deep_tree_clf1.fit(Xm, ym)\n",
    "deep_tree_clf2.fit(Xm, ym)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_decision_boundary(deep_tree_clf1, Xm, ym, axes=[-1.5, 2.4, -1, 1.5], iris=False)\n",
    "plt.title(\"No restrictions\", fontsize=16)\n",
    "plt.sca(axes[1])\n",
    "plot_decision_boundary(deep_tree_clf2, Xm, ym, axes=[-1.5, 2.4, -1, 1.5], iris=False)\n",
    "plt.title(\"min_samples_leaf = {}\".format(deep_tree_clf2.min_samples_leaf), fontsize=14)\n",
    "plt.ylabel(\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-class challenge 1\n",
    "- Below, you will find a line loading another inbuilt dataset from `sklearn`: the `wine` data. This data set is the result of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.\n",
    "\n",
    "- Use code from above to fit a Decision Tree to this data, to determine which of the 13 measured predictors are to be used to classify some wine into one of the three categories. *Make sure to use `train_test_split` to split your data into training and test datasets!* \n",
    "\n",
    "- What difference do the different regularisation parameters make?\n",
    "\n",
    "- What's your prediction success rate? What does this say about the data? Run your Decision Tree a few times to see how the random number generator plays a big role! \n",
    "\n",
    "- *Hint:* plotting the decision tree may or may not end up a little confusing and unclear. You can use `print(export_tree(..., feature_names=...))` to get a text formatted version instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = sklearn.datasets.load_wine()\n",
    "# print(wine.DESCR) # Uncomment to get detailed dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1, 2, 3, code!\n",
    "X_wine, y_wine = load_wine(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_wine, y_wine)\n",
    "\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "print(export_text(tree_clf))\n",
    "\n",
    "np.sum(tree_clf.predict(X_test) == y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Extra material: Decision tree regression\n",
    "\n",
    "- Whilst decision trees are mostly used for classification, they can also be used for regression, as you can see below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic training set + noise\n",
    "m = 200\n",
    "X = np.random.rand(m, 1)\n",
    "y = 4 * (X - 0.5) ** 2\n",
    "y = y + np.random.randn(m, 1) / 10\n",
    "\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=3)\n",
    "tree_reg1.fit(X, y)\n",
    "tree_reg2.fit(X, y)\n",
    "\n",
    "def plot_regression_predictions(tree_reg, X, y, axes=[0, 1, -0.2, 1], ylabel=\"$y$\"):\n",
    "    x1 = np.linspace(axes[0], axes[1], 500).reshape(-1, 1)\n",
    "    y_pred = tree_reg.predict(x1)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    if ylabel:\n",
    "        plt.ylabel(ylabel, fontsize=18, rotation=0)\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    plt.plot(x1, y_pred, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(11, 5), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_regression_predictions(tree_reg1, X, y)\n",
    "#for split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\n",
    "    #plt.plot([split, split], [-0.2, 1], style, linewidth=2)\n",
    "#plt.text(0.21, 0.65, \"Depth=0\", fontsize=15)\n",
    "#plt.text(0.01, 0.2, \"Depth=1\", fontsize=13)\n",
    "#plt.text(0.65, 0.8, \"Depth=1\", fontsize=13)\n",
    "plt.legend(loc=\"upper center\", fontsize=18)\n",
    "plt.title(\"max_depth=2\", fontsize=14)\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plot_regression_predictions(tree_reg2, X, y, ylabel=None)\n",
    "#for split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\n",
    "    #plt.plot([split, split], [-0.2, 1], style, linewidth=2)\n",
    "#for split in (0.0458, 0.1298, 0.2873, 0.9040):\n",
    "    #plt.plot([split, split], [-0.2, 1], \"k:\", linewidth=1)\n",
    "#plt.text(0.3, 0.5, \"Depth=2\", fontsize=13)\n",
    "plt.title(\"max_depth=3\", fontsize=14)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree(\n",
    "        tree_reg1,\n",
    "        feature_names=[\"x1\"],\n",
    "        rounded=True,\n",
    "        filled=True\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree_reg1 = DecisionTreeRegressor()\n",
    "tree_reg2 = DecisionTreeRegressor(min_samples_leaf=10)\n",
    "tree_reg1.fit(X, y)\n",
    "tree_reg2.fit(X, y)\n",
    "\n",
    "x1 = np.linspace(0, 1, 500).reshape(-1, 1)\n",
    "y_pred1 = tree_reg1.predict(x1)\n",
    "y_pred2 = tree_reg2.predict(x1)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(x1, y_pred1, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "plt.axis([0, 1, -0.2, 1.1])\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", fontsize=18, rotation=0)\n",
    "plt.legend(loc=\"upper center\", fontsize=18)\n",
    "plt.title(\"No restrictions\", fontsize=14)\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(x1, y_pred2, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "plt.axis([0, 1, -0.2, 1.1])\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.title(\"min_samples_leaf={}\".format(tree_reg2.min_samples_leaf), fontsize=14)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Ensemble Learning: Random Forests\n",
    "\n",
    "- Ensemble learning stems from the concept that aggregating many predictions of a group of predictors gives better predictions that just the best individual predictor (*wisdom of the crowd*).\n",
    "\n",
    "<img src=\"https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0701.png\" alt=\"Ensemble learning\" style=\"width:60%\" />\n",
    "\n",
    "- A *random forest* is an ensemble method build from a group of Decision Trees trained on a different random subset of the training data. \n",
    "- To predict, obtain the predictions of all individual trees and choose the option with the most votes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Learning\n",
    "\n",
    "Ensemble methods work best when the predictors are as **independent**\n",
    "from each other as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=500, noise=0.30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
    "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
    "plt.xlabel(r\"$x_1$\", fontsize=20)\n",
    "plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(solver=\"lbfgs\")\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100)\n",
    "svm_clf = SVC(gamma=\"scale\", probability=True)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard') # Change this to 'soft' and see what happens!\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print( accuracy_score(y_test, y_pred), clf.__class__.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Soft learning* uses the averaged class probabilities instead of hard predictions. Try this out by changing the voting mechanism in the `VotingClassifier` to `'soft'`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging & Pasting\n",
    "\n",
    "- Ensemble methods can work either by using a diverse range of prediction algorithms or by using the same algorithm and **training it on different random subsets** of the training data.\n",
    "- When sampling is performed *with* replacement, this method is called **bagging** (short for bootstrap aggregating). When sampling is performed *without* replacement, it is called **pasting**.\n",
    "- Predictions are typically performed by taking the most frequent prediction or the average of all predictions. \n",
    "\n",
    "<!--- <img src=\"https://hackernoon.com/hn-images/0*zrm9Q8twgrq8lfLk.\" alt=\"bagging\" style=\"width:80%\" align=\"middle\" /> -->\n",
    "\n",
    "<img src=\"https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0704.png\" alt=\"bagging and pasting\" style=\"width:60%\" align=\"middle\"/>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "print(\"One single tree:\", accuracy_score(y_test, y_pred_tree))\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, n_jobs=-1)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "print(\"A whole forest:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, axes = plt.subplots(ncols=2, figsize=(10,4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_decision_boundary(tree_clf, X, y, axes=[-1.5, 2.45, -1, 1.5], iris = False)\n",
    "plt.title(\"Decision Tree\", fontsize=14)\n",
    "plt.sca(axes[1])\n",
    "plot_decision_boundary(bag_clf, X, y, axes=[-1.5, 2.45, -1, 1.5], iris = False)\n",
    "plt.title(\"Decision Trees with Bagging\", fontsize=14)\n",
    "plt.ylabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forests\n",
    "\n",
    "- This is simply an *ensemble of decision trees*, usually using the *bagging* method with `max_samples` set to the same as the size of the training dataset.\n",
    "- The two codes below are almost identical: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This: \n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
    "    n_estimators=500, max_samples=1.0, bootstrap=True)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "\n",
    "# is almost identical to: \n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16)\n",
    "\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "\n",
    "# Let's compare:\n",
    "np.sum(y_pred == y_pred_rf) / len(y_pred)  # almost identical predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A great advantage of random forests is that they can assign importance to each feature by looking at how much the tree nodes that use that feature reduce impurity on average (across all trees in the forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-class challenge 2\n",
    "\n",
    "- Remember the wine dataset? The Decision Tree earlier on worked ok, but the prediction success rate varied wildly and wasn't all that good either... \n",
    "- What about fitting a Random Forest? *Again, don't forget to split training and test!*\n",
    "- Which of the features were the most important in the predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's start by loading the wine dataset\n",
    "X_wine, y_wine = load_wine(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_wine, y_wine)\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500)\n",
    "\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_pred_rf, y_test))\n",
    "for name, score in zip(load_wine()[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(round(score,2), name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Material: Stacking \n",
    "<img src=\"https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0712.png\" alt=\"stacking\" style=\"width:40%\" align=\"left\" />  \n",
    "- Simple: instead of using trivial functions (such as hard voting) to aggregate the predictions of all predictors in an ensemble, train a model to perform this aggregation!\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"https://1.bp.blogspot.com/-tgUf5CDQm8A/XQfjAId11cI/AAAAAAAARC8/P7MG69COxh8mMjuJaL0NNLXqQgmvNzcawCLcBGAs/s1600/64369119_2391298877618340_4851478784206962688_n.jpg\" alt=\"stacking\" style=\"width:40%\" align=\"right\" />  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to train this? Split the training dataset into two halves and train layer 1 (\"Predict\" in the graphic above) using one half and layer 2 (\"Blending\") using the other half. \n",
    "\n",
    "\n",
    "<img src=\"https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0713.png\" alt=\"training stacking 1\" style=\"width:50%\" align=\"left\">\n",
    "<img src=\"https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0714.png\" alt=\"training stacking 2\" style=\"width:40%\" align=\"right\">\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, Sci-kit learn does not have an inbuild function for this. See the code below for an implementation of stacking. (Unless your Sci-kit learn version >= 0.22, see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=10000, noise=0.3)\n",
    "\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
    "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
    "plt.grid(True, which='both')\n",
    "plt.xlabel(r\"$x_1$\", fontsize=20)\n",
    "plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=1000)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proceed with care: the following code may run for *a while*..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_clf = RandomForestClassifier(n_estimators=100)\n",
    "extra_trees_clf = ExtraTreesClassifier(n_estimators=100)\n",
    "svm_clf = LinearSVC()\n",
    "mlp_clf = MLPClassifier(max_iter = 1000)\n",
    "\n",
    "estimators = [random_forest_clf, extra_trees_clf, svm_clf, mlp_clf]\n",
    "for estimator in estimators:\n",
    "    print(\"Training the\", estimator)\n",
    "    estimator.fit(X_train, y_train)\n",
    "    \n",
    "print(\"Accuracies:\", [estimator.score(X_val, y_val) for estimator in estimators])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_predictions = np.empty((len(X_val), len(estimators)), dtype=np.float32)\n",
    "\n",
    "for index, estimator in enumerate(estimators):\n",
    "        X_val_predictions[:, index] = estimator.predict(X_val)\n",
    "    \n",
    "X_val_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_forest_blender = RandomForestClassifier(n_estimators=200, oob_score=True)\n",
    "rnd_forest_blender.fit(X_val_predictions, y_val)\n",
    "\n",
    "\n",
    "X_test_predictions = np.empty((len(X_test), len(estimators)), dtype=np.float32)\n",
    "\n",
    "for index, estimator in enumerate(estimators):\n",
    "    X_test_predictions[:, index] = estimator.predict(X_test)\n",
    "    \n",
    "y_pred = rnd_forest_blender.predict(X_test_predictions)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly in this case, the `MLPClassifier` actually does better on its own! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sklearn.__version__ >= \"0.22\" :\n",
    "    from sklearn.ensemble import StackingClassifier\n",
    "else: print(sklearn.__version__)\n",
    "\n",
    "def stacking():\n",
    "    # define the base models\n",
    "    level0 = list()\n",
    "    level0.append(('rf', RandomForestClassifier(n_estimators=100)))\n",
    "    level0.append(('etr', ExtraTreesClassifier(n_estimators=100)))\n",
    "    level0.append(('svm', LinearSVC()))\n",
    "    level0.append(('mlp', MLPClassifier(max_iter = 1000)))\n",
    "    # define meta learner model\n",
    "    level1 = RandomForestClassifier(n_estimators=200, oob_score=True)\n",
    "    # define the stacking ensemble\n",
    "    model = StackingClassifier(estimators=level0, final_estimator=level1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_model = stacking().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = stacking_model.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extra Material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "- A powerful and versatile machine learning model\n",
    "- Well-suited for classifying small- and medium-sized datasets\n",
    "- Capable of performing:\n",
    "    * Linear & Non-linear classification\n",
    "    * Regression\n",
    "    * Outlier detection\n",
    "    \n",
    "A good resource for more information is sklearn's own documentation on SVMs, to be found here: https://scikit-learn.org/stable/modules/svm.html\n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/7/72/SVM_margin.png\" alt=\"SVM\" width=\"300\" align=\"right\"/>\n",
    "\n",
    "### Linear SVM Classification\n",
    "\n",
    "- Objective is to find a line (i.e. collection of points) satisfying $ w^Tx + b=0$, the **decision boundary**\n",
    "- Predictions:  \n",
    "$ \\hat y = \\begin{cases}\n",
    "      \\text{class A} & \\text{if $w^T x + b < 0$}\\\\\n",
    "      \\text{class B} & \\text{if $w^T x + b >= 0$}    \n",
    "    \\end{cases}       \n",
    "$\n",
    "- Important to note: SVMs only provide binary predictions, as opposed to probabilistic values like in logistic regressions!\n",
    "\n",
    "- In the following, we see an example of this: \n",
    "    - Again, we use the iris dataset\n",
    "    - However, we only use a subset of the data here: Iris setosa and Iris versicolor only, no Iris virginica\n",
    "    - Use the `SVC` command, with `kernel=\"linear\"` to fit a simple linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = iris[\"target\"]\n",
    "\n",
    "setosa_or_versicolor = (y == 0) | (y == 1)\n",
    "X = X[setosa_or_versicolor]\n",
    "y = y[setosa_or_versicolor]\n",
    "\n",
    "# SVM Classifier model\n",
    "svm_clf = SVC(kernel=\"linear\", C=float(\"inf\"))\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad models\n",
    "x0 = np.linspace(0, 5.5, 200)\n",
    "pred_1 = 5*x0 - 20\n",
    "pred_2 = x0 - 1.8\n",
    "pred_3 = 0.1 * x0 + 0.5\n",
    "\n",
    "def plot_svc_decision_boundary(svm_clf, xmin, xmax):\n",
    "    w = svm_clf.coef_[0]\n",
    "    b = svm_clf.intercept_[0]\n",
    "\n",
    "    # At the decision boundary, w0*x0 + w1*x1 + b = 0\n",
    "    # => x1 = -w0/w1 * x0 - b/w1\n",
    "    x0 = np.linspace(xmin, xmax, 200)\n",
    "    decision_boundary = -w[0]/w[1] * x0 - b/w[1]\n",
    "\n",
    "    margin = 1/w[1]\n",
    "    gutter_up = decision_boundary + margin\n",
    "    gutter_down = decision_boundary - margin\n",
    "\n",
    "    svs = svm_clf.support_vectors_\n",
    "    plt.scatter(svs[:, 0], svs[:, 1], s=180, facecolors='#FFAAAA')\n",
    "    plt.plot(x0, decision_boundary, \"k-\", linewidth=2)\n",
    "    plt.plot(x0, gutter_up, \"k--\", linewidth=2)\n",
    "    plt.plot(x0, gutter_down, \"k--\", linewidth=2)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10,2.7), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(x0, pred_1, \"g--\", linewidth=2)\n",
    "plt.plot(x0, pred_2, \"m-\", linewidth=2)\n",
    "plt.plot(x0, pred_3, \"r-\", linewidth=2)\n",
    "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris versicolor\")\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris setosa\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([0, 5.5, 0, 2])\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plot_svc_decision_boundary(svm_clf, 0, 5.5)\n",
    "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\")\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.axis([0, 5.5, 0, 2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots, we can see how a SVM works:\n",
    "- The plot on the left shows: \n",
    "    * decision boundaries of three possible linear classifiers\n",
    "    * both purple and red classifiers are good boundaries for the data\n",
    "    * the dashed green line is so bad that it does not even separate the classes properly\n",
    "- The plot on the right shows how an SVM works:\n",
    "    * The solid black line represents the decision boundary of an SVM classifier\n",
    "    * This line stays as far away from the closest training instances as possible\n",
    "    * Think of fitting the widest possible street (represented by the parallel dashed lines)\n",
    "    \n",
    "    \n",
    "### Sensitivity to feature scales\n",
    "\n",
    "In the left plot below, the vertical scale is much larger than the horizontal scale, so the widest possible street is close to horizontal. After feature scaling (e.g., using Scikit-Learn’s `StandardScaler`), the decision boundary in the right plot looks much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = np.array([[1, 50], [5, 20], [3, 80], [5, 60]]).astype(np.float64)\n",
    "ys = np.array([0, 0, 1, 1])\n",
    "svm_clf = SVC(kernel=\"linear\", C=100)\n",
    "svm_clf.fit(Xs, ys)\n",
    "\n",
    "plt.figure(figsize=(9,2.7))\n",
    "plt.subplot(121)\n",
    "plt.plot(Xs[:, 0][ys==1], Xs[:, 1][ys==1], \"bo\")\n",
    "plt.plot(Xs[:, 0][ys==0], Xs[:, 1][ys==0], \"ms\")\n",
    "plot_svc_decision_boundary(svm_clf, 0, 6)\n",
    "plt.xlabel(\"$x_0$\", fontsize=20)\n",
    "plt.ylabel(\"$x_1$    \", fontsize=20, rotation=0)\n",
    "plt.title(\"Unscaled\", fontsize=16)\n",
    "plt.axis([0, 6, 0, 90])\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(Xs)\n",
    "svm_clf.fit(X_scaled, ys)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(X_scaled[:, 0][ys==1], X_scaled[:, 1][ys==1], \"bo\")\n",
    "plt.plot(X_scaled[:, 0][ys==0], X_scaled[:, 1][ys==0], \"ms\")\n",
    "plot_svc_decision_boundary(svm_clf, -2, 2)\n",
    "plt.xlabel(\"$x_0$\", fontsize=20)\n",
    "plt.ylabel(\"$x'_1$  \", fontsize=20, rotation=0)\n",
    "plt.title(\"Scaled\", fontsize=16)\n",
    "plt.axis([-2, 2, -2, 2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity to outliers\n",
    "\n",
    "Linear SVMs only work when the data is linearly separable. They are also sensitive to outliers as you can see from the second plot below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_outliers = np.array([[3.4, 1.3], [3.2, 0.8]])\n",
    "y_outliers = np.array([0, 0])\n",
    "Xo1 = np.concatenate([X, X_outliers[:1]], axis=0)\n",
    "yo1 = np.concatenate([y, y_outliers[:1]], axis=0)\n",
    "Xo2 = np.concatenate([X, X_outliers[1:]], axis=0)\n",
    "yo2 = np.concatenate([y, y_outliers[1:]], axis=0)\n",
    "\n",
    "svm_clf2 = SVC(kernel=\"linear\", C=10**9)\n",
    "svm_clf2.fit(Xo2, yo2)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10,2.7), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(Xo1[:, 0][yo1==1], Xo1[:, 1][yo1==1], \"bs\")\n",
    "plt.plot(Xo1[:, 0][yo1==0], Xo1[:, 1][yo1==0], \"yo\")\n",
    "plt.text(0.3, 1.0, \"Impossible!\", fontsize=20, color=\"red\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.annotate(\"Outlier\",\n",
    "             xy=(X_outliers[0][0], X_outliers[0][1]),\n",
    "             xytext=(2.5, 1.7),\n",
    "             ha=\"center\",\n",
    "             arrowprops=dict(facecolor='black', shrink=0.1),\n",
    "             fontsize=16,\n",
    "            )\n",
    "plt.axis([0, 5.5, 0, 2])\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.plot(Xo2[:, 0][yo2==1], Xo2[:, 1][yo2==1], \"bs\")\n",
    "plt.plot(Xo2[:, 0][yo2==0], Xo2[:, 1][yo2==0], \"yo\")\n",
    "plot_svc_decision_boundary(svm_clf2, 0, 5.5)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.annotate(\"Outlier\",\n",
    "             xy=(X_outliers[1][0], X_outliers[1][1]),\n",
    "             xytext=(3.2, 0.08),\n",
    "             ha=\"center\",\n",
    "             arrowprops=dict(facecolor='black', shrink=0.1),\n",
    "             fontsize=16,\n",
    "            )\n",
    "plt.axis([0, 5.5, 0, 2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large margin *vs* margin violations \n",
    "\n",
    "- To avoid these issues, use *soft margin classification*\n",
    "- Objective: find a good balance between width of decision boundary and *margin violations*\n",
    "- This can be achieved through the `hinge` loss function, for a plot, see the first cell below.\n",
    "- `sklearn`'s `LinearSVC` function also takes a hyperparameter: `C`. This is similar to the identically named hyperparameter in `LogisticRegression` and represents the tradeoff between misclassification and simplicity of the decision boundary. In general, the larger `C`, the fewer margin violations but the smaller the margin. Beware of overfitting! A value of `C=1.0` is generally a good start. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64)  # Iris virginica\n",
    "\n",
    "scaler = StandardScaler()\n",
    "svm_clf1 = LinearSVC(C=1, loss=\"hinge\")\n",
    "svm_clf2 = LinearSVC(C=100, loss=\"hinge\")\n",
    "\n",
    "scaled_svm_clf1 = Pipeline([\n",
    "        (\"scaler\", scaler),\n",
    "        (\"linear_svc\", svm_clf1),\n",
    "    ])\n",
    "scaled_svm_clf2 = Pipeline([\n",
    "        (\"scaler\", scaler),\n",
    "        (\"linear_svc\", svm_clf2),\n",
    "    ])\n",
    "\n",
    "scaled_svm_clf1.fit(X, y)\n",
    "scaled_svm_clf2.fit(X, y)\n",
    "\n",
    "# Convert to unscaled parameters\n",
    "b1 = svm_clf1.decision_function([-scaler.mean_ / scaler.scale_])\n",
    "b2 = svm_clf2.decision_function([-scaler.mean_ / scaler.scale_])\n",
    "w1 = svm_clf1.coef_[0] / scaler.scale_\n",
    "w2 = svm_clf2.coef_[0] / scaler.scale_\n",
    "svm_clf1.intercept_ = np.array([b1])\n",
    "svm_clf2.intercept_ = np.array([b2])\n",
    "svm_clf1.coef_ = np.array([w1])\n",
    "svm_clf2.coef_ = np.array([w2])\n",
    "\n",
    "# Find support vectors (LinearSVC does not do this automatically)\n",
    "t = y * 2 - 1\n",
    "support_vectors_idx1 = (t * (X.dot(w1) + b1) < 1).ravel()\n",
    "support_vectors_idx2 = (t * (X.dot(w2) + b2) < 1).ravel()\n",
    "svm_clf1.support_vectors_ = X[support_vectors_idx1]\n",
    "svm_clf2.support_vectors_ = X[support_vectors_idx2]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10,2.7), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\", label=\"Iris virginica\")\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\", label=\"Iris versicolor\")\n",
    "plot_svc_decision_boundary(svm_clf1, 4, 5.9)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.title(\"$C = {}$\".format(svm_clf1.C), fontsize=16)\n",
    "plt.axis([4, 5.9, 0.8, 2.8])\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
    "plot_svc_decision_boundary(svm_clf2, 4, 5.99)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.title(\"$C = {}$\".format(svm_clf2.C), fontsize=16)\n",
    "plt.axis([4, 5.9, 0.8, 2.8])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Non-linear classification\n",
    "\n",
    "What if data is not *linearly separable*? One approach would be to add more features, e.g. polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1D = np.linspace(-4, 4, 9).reshape(-1, 1)\n",
    "X2D = np.c_[X1D, X1D**2]\n",
    "y = np.array([0, 0, 1, 1, 1, 1, 1, 0, 0])\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.grid(True, which='both')\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.plot(X1D[:, 0][y==0], np.zeros(4), \"bs\")\n",
    "plt.plot(X1D[:, 0][y==1], np.zeros(5), \"g^\")\n",
    "plt.gca().get_yaxis().set_ticks([])\n",
    "plt.xlabel(r\"$x_1$\", fontsize=20)\n",
    "plt.axis([-4.5, 4.5, -0.2, 0.2])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.grid(True, which='both')\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.axvline(x=0, color='k')\n",
    "plt.plot(X2D[:, 0][y==0], X2D[:, 1][y==0], \"bs\")\n",
    "plt.plot(X2D[:, 0][y==1], X2D[:, 1][y==1], \"g^\")\n",
    "plt.xlabel(r\"$x_1$\", fontsize=20)\n",
    "plt.ylabel(r\"$x_2$  \", fontsize=20, rotation=0)\n",
    "plt.gca().get_yaxis().set_ticks([0, 4, 8, 12, 16])\n",
    "plt.plot([-4.5, 4.5], [6.5, 6.5], \"r--\", linewidth=3)\n",
    "plt.axis([-4.5, 4.5, -1, 17])\n",
    "\n",
    "plt.subplots_adjust(right=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate this idea on a more complex data set, let's consider the moons dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=100, noise=0.15)\n",
    "\n",
    "def plot_dataset(X, y, axes):\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
    "    plt.axis(axes)\n",
    "    plt.grid(True, which='both')\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=20)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n",
    "\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `sklearn`, we can use `PolynomialFeatures`, just like in the polynomial regressions from last time, to add additional features to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "polynomial_svm_clf = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", max_iter=10**5))\n",
    "    ])\n",
    "\n",
    "polynomial_svm_clf.fit(X, y)\n",
    "\n",
    "def plot_predictions(clf, axes):\n",
    "    x0s = np.linspace(axes[0], axes[1], 100)\n",
    "    x1s = np.linspace(axes[2], axes[3], 100)\n",
    "    x0, x1 = np.meshgrid(x0s, x1s)\n",
    "    X = np.c_[x0.ravel(), x1.ravel()]\n",
    "    y_pred = clf.predict(X).reshape(x0.shape)\n",
    "    y_decision = clf.decision_function(X).reshape(x0.shape)\n",
    "    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n",
    "    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n",
    "\n",
    "plot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The kernel trick\n",
    "\n",
    "High-order polynomial features will cause the number of features to explode. This problem can be solved through the *kernel trick*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, r, C = 5, 1, 5 # Play around with these values!\n",
    "\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"poly\", degree=d, coef0=r, C=C))\n",
    "    ])\n",
    "poly_kernel_svm_clf.fit(X, y)\n",
    "\n",
    "plot_predictions(poly_kernel_svm_clf, [-1.5, 2.45, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.4, -1, 1.5])\n",
    "plt.title(r\"$d={}, r={}, C={}$\".format(d,r,C), fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Gaussian RBF Kernel for Support Vector Machines\n",
    "\n",
    "- Another technique to tackle nonlinear problems is to add features computed using a *similarity function*, which measures how much each instance resembles a particular landmark. \n",
    "- In the case of the Gaussian Radial Basis Function (RBF): \n",
    "$\n",
    "{\\displaystyle \\phi_{\\gamma}(\\mathbf{x}, \\boldsymbol{\\ell})} = {\\displaystyle \\exp({\\displaystyle -\\gamma \\left\\| \\mathbf{x} - \\boldsymbol{\\ell} \\right\\|^2})}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gaussian_rbf(x, landmark, gamma):\n",
    "    return np.exp(-gamma * np.linalg.norm(x - landmark, axis=1)**2)\n",
    "\n",
    "gamma = 0.3\n",
    "landmark1 = -2\n",
    "landmark2 = 1\n",
    "\n",
    "x1s = np.linspace(-4.5, 4.5, 200).reshape(-1, 1)\n",
    "x2s = gaussian_rbf(x1s, landmark1, gamma)\n",
    "x3s = gaussian_rbf(x1s, landmark2, gamma)\n",
    "\n",
    "X1D = np.linspace(-4, 4, 9).reshape(-1, 1)\n",
    "XK = np.c_[gaussian_rbf(X1D, landmark1, gamma), \n",
    "           gaussian_rbf(X1D, landmark2, gamma)]\n",
    "yk = np.array([0, 0, 1, 1, 1, 1, 1, 0, 0])\n",
    "\n",
    "plt.figure(figsize=(10.5, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.grid(True, which='both')\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.axvline(x=-1, color='k', ls=\"--\")\n",
    "plt.scatter(x=[landmark1, landmark2, -1, -1], \n",
    "            y=[0, 0,  \n",
    "               gaussian_rbf(np.array([[-1]]), landmark1, gamma)[0], \n",
    "               gaussian_rbf(np.array([[-1]]), landmark2, gamma)[0]], \n",
    "            s=150, alpha=0.5, c=\"red\")\n",
    "plt.plot(X1D[:, 0][yk==0], np.zeros(4), \"bs\")\n",
    "plt.plot(X1D[:, 0][yk==1], np.zeros(5), \"g^\")\n",
    "plt.plot(x1s, x2s, \"g--\")\n",
    "plt.plot(x1s, x3s, \"b:\")\n",
    "plt.gca().get_yaxis().set_ticks([0, 0.25, 0.5, 0.75, 1])\n",
    "plt.xlabel(r\"$x_1$\", fontsize=20)\n",
    "plt.ylabel(r\"Similarity\", fontsize=14)\n",
    "plt.annotate(r'$\\mathbf{x}$',\n",
    "             xy=(X1D[3, 0], 0),\n",
    "             xytext=(-0.5, 0.20),\n",
    "             ha=\"center\",\n",
    "             arrowprops=dict(facecolor='black', shrink=0.1),\n",
    "             fontsize=18,\n",
    "            )\n",
    "plt.text(-2, 0.9, \"$x_2$\", ha=\"center\", fontsize=20)\n",
    "plt.text(1, 0.9, \"$x_3$\", ha=\"center\", fontsize=20)\n",
    "plt.axis([-4.5, 4.5, -0.1, 1.1])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.grid(True, which='both')\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.axvline(x=0, color='k')\n",
    "plt.plot(XK[:, 0][yk==0], XK[:, 1][yk==0], \"bs\")\n",
    "plt.plot(XK[:, 0][yk==1], XK[:, 1][yk==1], \"g^\")\n",
    "plt.xlabel(r\"$x_2$\", fontsize=20)\n",
    "plt.ylabel(r\"$x_3$  \", fontsize=20, rotation=0)\n",
    "plt.annotate(r'$\\phi\\left(\\mathbf{x}\\right)$',\n",
    "             xy=(XK[3, 0], XK[3, 1]),\n",
    "             xytext=(0.65, 0.50),\n",
    "             ha=\"center\",\n",
    "             arrowprops=dict(facecolor='black', shrink=0.1),\n",
    "             fontsize=18,\n",
    "            )\n",
    "plt.plot([-0.1, 1.1], [0.57, -0.1], \"r--\", linewidth=3)\n",
    "plt.axis([-0.1, 1.1, -0.1, 1.1])\n",
    "    \n",
    "plt.subplots_adjust(right=1)\n",
    "plt.show()\n",
    "\n",
    "x1_example = X1D[3, 0]\n",
    "for landmark in (landmark1, landmark2):\n",
    "    k = gaussian_rbf(np.array([[x1_example]]), np.array([[landmark]]), gamma)\n",
    "    print(\"Phi({}, {}) = {}\".format(x1_example, landmark, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=100, noise=0.15)\n",
    "\n",
    "rbf_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "    ])\n",
    "rbf_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma1, gamma2 = 0.1, 5\n",
    "C1, C2 = 0.001, 1000\n",
    "hyperparams = (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2)\n",
    "\n",
    "svm_clfs = []\n",
    "for gamma, C in hyperparams:\n",
    "    rbf_kernel_svm_clf = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"svm_clf\", SVC(kernel=\"rbf\", gamma=gamma, C=C))\n",
    "        ])\n",
    "    rbf_kernel_svm_clf.fit(X, y)\n",
    "    svm_clfs.append(rbf_kernel_svm_clf)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10.5, 7), sharex=True, sharey=True)\n",
    "\n",
    "for i, svm_clf in enumerate(svm_clfs):\n",
    "    plt.sca(axes[i // 2, i % 2])\n",
    "    plot_predictions(svm_clf, [-1.5, 2.45, -1, 1.5])\n",
    "    plot_dataset(X, y, [-1.5, 2.45, -1, 1.5])\n",
    "    gamma, C = hyperparams[i]\n",
    "    plt.title(r\"$\\gamma = {}, C = {}$\".format(gamma, C), fontsize=16)\n",
    "    if i in (0, 1):\n",
    "        plt.xlabel(\"\")\n",
    "    if i in (1, 3):\n",
    "        plt.ylabel(\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take-home challenge: Classifying biodegradability\n",
    "\n",
    "- This week, we will look at the QSAR biodegradation dataset available [here](https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation) which can also be found in the datasets folder in this workspace. This dataset contains 41 molecular descriptors and 1 experimental class: ready biodegradable (RB) and not ready biodegradable (NRB). As you can tell, this dataset lends itself nicely to using decision trees and random forests. \n",
    "- Firstly, have a look at the data webpage linked above. Familiarise yourself with all the descriptors and think about which ones may have more predicting power over others. \n",
    "- Using a package of your choice (`csv` or `pandas` or something else), read in the dataset and encode the class variable as integers. The direct link to the dataset is here (this works directly with pandas.read_csv!): `https://archive.ics.uci.edu/ml/machine-learning-databases/00254/biodeg.csv` \n",
    "- Fit a decision tree (*training/test*!). What's the prediction success rate? Find out the spread of the success rate (mean, variance).\n",
    "- Fit a random forest. Does this improve things? \n",
    "- Can you tune the parameters to get an even better prediction rate? \n",
    "- Which parameters are more important than others? What happens when you train on a subset of parameters? Play around with this to see what happens. \n",
    "- *Bonus:* In the associated paper, the authors used - amongst other methods - Support Vector Machines fitted to the data. Read to the extra material on SVMs and try to fit an SVM to this data. How does it perform compared to the random forest?  \n",
    "- *Bonus 2:* Implement a simple stacking ensemble method with your choice of predictors. Again, can you improve your high-score?\n",
    "- *Bonus 3:* Using at least 5-fold cross-validation (https://scikit-learn.org/stable/modules/cross_validation.html) to obtain a range of predictions, report your average precision for your best model. *Hint: if you'd like to use a Stacking classifier, have a look at the `mlxtend` package.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

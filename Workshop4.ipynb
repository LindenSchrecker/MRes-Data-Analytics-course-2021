{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/FlorianSong/MResAMS_DataAnalytics/blob/main/Workshop4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analytics 2021\n",
    "### Workshop 4 &ndash; Artificial Neural Networks: a whistle stop introduction &ndash; 25th November 2021\n",
    "##### Taught by: Linden Schrecker, Annabel Basford, Nan Wu, Sophia Yaliraki\n",
    "##### Written by: Florian Song \n",
    "\n",
    "Much of today's workshop was taken from https://github.com/ageron/handson-ml2/ which in turn is based on the second edition of an O'Reilly book [Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) by AurÃ©lien Geron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview & Outline\n",
    "\n",
    "\n",
    "#### Topics covered today:\n",
    "0. Choosing the right estimator\n",
    "1. Artificial Neural Networks \n",
    "    * Biological vs artificial neurons\n",
    "    * Perceptrons\n",
    "    * Multi-layer perceptrons\n",
    "1. Implementation using TensorFlow and the Keras API\n",
    "    * Building an image classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, some house-keeping. We need to check if we have all the necessary libraries installed. For this we can use the following boiler plate code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python â‰¥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn â‰¥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow â‰¥2.0 is required\n",
    "#try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    #%tensorflow_version 2.x\n",
    "#except Exception:\n",
    "    #pass\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Keras â‰¥2.4 is required\n",
    "from tensorflow import keras\n",
    "print(\"Keras version:\", keras.__version__) \n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "# np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Sci-kit Learn\n",
    "from sklearn.datasets import load_iris, load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Choosing the right estimator\n",
    "Often the hardest part of solving a machine learning problem can be finding the right estimator for the job.\n",
    "Different estimators are better suited for different types of data and different problems.\n",
    "The flowchart below is designed to give users a bit of a rough guide on how to approach problems with regard to which estimators to try on your data. Source (interactive!): https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n",
    "\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_static/ml_map.png\" alt=\"scikit learn cheat sheet\" width=\"90%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Artificial Neural Networks\n",
    "\n",
    "- Many modern technologies are inspired by nature: Birds taught us to fly, burrs of burdock motivated the invention of velcro, bats' and dolphins' ability to use echolocation led to sonars and so on. It therefore seems logical to turn to the **brain's architecture to seek inspiration for intelligent machines**. \n",
    "- An *Artificial Neural Network (ANN)* is a Machine Learning model inspired by the networks of biological neurons found in our brains. \n",
    "\n",
    "- A short history:\n",
    "    * First introduced in 1943 by neurophysiologist Warren McCulloch and mathematician Walter Pitts in the landmark paper [\"A logical calculus of the ideas immanent in nervous activity\"](https://doi.org/10.1007/BF02478259).\n",
    "    * With the widespread belief that we would soon be conversing with intelligent machines, an initial hype period ensued, until the 1960s when the hype died down and funding went dry.\n",
    "    * After a long hibernation, the 1980s saw a slow revival as new architectures and better training techniques became available, particular with the publication of the [\"Hopfield net\"](https://doi.org/10.1073/pnas.79.8.2554) in 1982 and the establishment of the \"Neural Networks for Computing\" conference by the American Institute of Physics in 1985. \n",
    "    * Until the 1990s, preference was mostly given to other ML techniques, such as Support Vector Machines as they provided better results.\n",
    "    * We are now in the midst of another wave of excitement, but will it last? Most people might argue that it will, with the advent of massive amounts of data, the increase in computing power and the continued theoretical development towards better algorithms. \n",
    "- ANNs are powerful and scalable and therefore able to: \n",
    "    * classify billions of images\n",
    "    * perform speech recognition\n",
    "    * play complex boardgames such as Go (https://deepmind.com/research/case-studies/alphago-the-story-so-far)\n",
    "    \n",
    "    \n",
    "### Biological vs artificial neurons \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/10/Blausen_0657_MultipolarNeuron.png\" alt=\"Biological neuron\" style=\"float: left; margin-right: 2%;\" width = \"30%\" />\n",
    "<img src=\"https://www.researchgate.net/profile/Kamel_Besbes/publication/228395588/figure/fig1/AS:302065948610575@1449029542802/The-basic-components-of-an-artificial-neuron.png\"  alt=\"Artificial neuron\" style=\"float: right; margin-left: 2%;\" width = \"30%\" />\n",
    "\n",
    "| Biological neurons | Artificial neurons | \n",
    "| :--- | ---: | \n",
    "| Composed of *dendrites*, a *cell body* and an *axon*.  | Composed of *input connections + synaptic weights*, a *summing junction* and an *activation function*. | \n",
    "| Sends signals in the form of *neurotransmitters* (chemicals) into the *synaptic terminals* | Sends signals in the form of *numbers* (in 1943: binary on/off) into the *output connection* |\n",
    "| Activates when *cell body* receives a sufficient amount of signals through *dendrites* | Activates when sufficient number of *input connections* are active |\n",
    "\n",
    "Pictures: [source](https://commons.wikimedia.org/wiki/File:Blausen_0657_MultipolarNeuron.png),\n",
    "[source](https://www.researchgate.net/publication/228395588_A_HIGHLY_TIME-EFFICIENT_DIGITAL_MULTIPLIER_BASED_ON_THE_A2_BINARY_REPRESENTATION)\n",
    "\n",
    "\n",
    "**Brain teaser:** Think about how you could rephrase a logistic regression model in terms of a single artificial neuron! *Hint:* Sigmoid activation function. \n",
    "\n",
    "\n",
    "### Perceptrons \n",
    "\n",
    "<img src=\"https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1005.png\" alt=\"Perceptron\" width=\"40%\" style=\"float: right; margin-left: 3%;\"/>\n",
    "\n",
    "- Composed of many artificial neurons, aka *threshold logic units (TLU)*. \n",
    "- The most common *activation functions* (aka *step functions*, *transfer functions* etc) are *Heaviside step, Sigmoid, Tanh and ReLU (Rectified Linear Unit).*\n",
    "- Perceptrons are normally organised into *layers*. \n",
    "- The *input layer* normally consists of *input neurons*, which output their input without any computation, and a *bias neuron* which outputs 1 all the time. \n",
    "- If all neurons of one layer are connected to every neuron in the previous layer, that layer is a *dense layer*. \n",
    "\n",
    "In `sklearn`, this is implemented in  [`sklearn.linear_model.Perceptron`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html), but why would you use that if you could instead use: \n",
    "\n",
    "### Multi-layer Perceptrons\n",
    "\n",
    "<img src=\"https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1007.png\" alt=\"Multilayer Perceptron\" width=\"35%\" style=\"float: right; margin-left: 3%;\"/>\n",
    "\n",
    "- Composed of one *input layer*, **one or many** *hidden layers* and one *output layer*. \n",
    "- Normally, every layer includes a *bias neuron*. \n",
    "- When the number of layers becomes large (most commonly tens, or even hundreds nowadays), the model is usually considered a *deep neural network*, one of the most powerful class of machine learning algorithms today and is known commonly as ***Deep Learning***.  \n",
    "- Simple MLPs are usually trained using *backpropagation*, proposed [here](https://scholar.google.com/scholar?q=Learning+Internal+Representations+by+Error+Propagation+author%3Arumelhart) in 1985.\n",
    "\n",
    "In `sklearn`, this is implemented in [`sklearn.neural_network.MLPClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html). \n",
    "\n",
    "Below is a short example code on how to use this on a familiar dataset, iris (sigh ðŸ˜’):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "mlp_clf = MLPClassifier(max_iter=5000,hidden_layer_sizes=(100,50,30))\n",
    "print(mlp_clf)\n",
    "\n",
    "mlp_clf.fit(X_train, y_train)\n",
    "mlp_clf.predict(X_test)\n",
    "print(\"Accuracy:\".ljust(25), round(mlp_clf.score(X_test, y_test),3))\n",
    "\n",
    "\n",
    "scores = cross_val_score(mlp_clf, X, y, cv = 5)\n",
    "print(\"Cross-validated accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementation using TensorFlow and the Keras API\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png\" alt=\"Keras logo\" width=\"30%\" style=\"float: right; margin-left: 3%;\" />  \n",
    "\n",
    "- Keras is a high-level Deep Learning API for building, training and evaluating all sorts of neural networks. The documentation can be found at https://keras.io/.\n",
    "- To perform the heavy computations for large neural nets, Keras requires a computation backend. At present, you can choose from: **TensorFlow**, Microsoft Cognitive Toolkit and Theano. Here, we will use the former. Specifically, we will use a version of Keras that is inbuilt in TensorFlow, called `tensorflow.keras`. \n",
    "<img src=\"https://www.gstatic.com/devrel-devsite/prod/v82ddc984cee1a5dd6ee4c16cb38492e67eefd500032375cc23778d6489eec4cb/tensorflow/images/lockup.svg\" alt=\"Tensorflow logo\" width=\"30%\" style=\"float: right; margin-left: 3%;\" />  \n",
    "\n",
    "- TensorFlow, originally developed by Google Brain, is \"an end-to-end open source machine learning platform\". For more information, go to https://www.tensorflow.org/.\n",
    "- Aside: For a recently released toy example of the capabilities of TensorFlow, have a look at this entirely browser-run Machine Learning tool: https://teachablemachine.withgoogle.com/\n",
    "- The main contender to TensorFlow at the moment is Facebook's PyTorch (https://pytorch.org/) library. Luckily, both APIs are quite similar, so switching is usually fairly easy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Image Classifier\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/zalandoresearch/fashion-mnist/master/doc/img/fashion-mnist-sprite.png\" alt=\"Fashion MNIST\" width=30% style=\"float: right; margin: 0px 0px 0px 30px;\"/>\n",
    "\n",
    "- For the final toy example of this workshop, we will be doing image classification on the fashion MNIST dataset (originally from [Zalando Research](https://research.zalando.com/welcome/mission/research-projects/fashion-mnist/)), a popular \"Hello World\" dataset for Deep Learning.\n",
    "- This dataset contains a training set of 60,000 instances and a test set of 10,000 instances. Each instance is a 28x28 pixel grayscale image of one of 10 fashion items, where each pixel intensity is represented as a byte (0 to 255).\n",
    "- Let's start by loading the fashion MNIST dataset. Keras has a number of functions to load popular datasets in `keras.datasets`. The dataset is already split for you between a training set and a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train, X_test = X_train / 255., X_test / 255. # Scale data to range 0 to 1, rather than 0 to 255\n",
    "\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "print(\"X_train shape/dimensions:\".ljust(30), X_train.shape)\n",
    "print(\"y_train:\".ljust(30), y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first image in the training set is a boot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_names[y_train[0]])\n",
    "plt.imshow(X_train[0], cmap=\"gray\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_rows = 4\n",
    "n_cols = 10\n",
    "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        index = n_cols * row + col\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
    "        plt.axis('off')\n",
    "        plt.title(class_names[y_train[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First Step: Creating a model using the Sequential API\n",
    "<img src=\"https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/code/ch13/images/13_04.png?raw=true\" alt=\"Activation functions\" width=\"25%\" style=\"float: right; margin-left: 0%;\"/>\n",
    "\n",
    "<img src=\"https://1.bp.blogspot.com/-svLN4YXrP5M/XAIe-GdhhJI/AAAAAAAANOo/s-HVNoMD8xMXZc0J-1n6JD903DIvq7ygwCLcBGAs/s1600/n9fgba8b0qr01.png\" alt=\"Layers meme\" width=\"30%\" style=\"float: right; margin-left: 3%;\"/>\n",
    "\n",
    "- In the following, we will build a classification Multi-layer Perceptron with two hidden layers.\n",
    "- To do so, we use the `Sequential` model provided by `keras`, which builds a simple neural net where many *layers* (the basic building blocks) are connected in a sequential manner, i.e. each layer is only getting input from the layer immediately before it. \n",
    "- `keras` supports many different activation functions. For a list of some popular ones, see the figure on the far right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session() # This ensures that any previous runs don't interfere.\n",
    "#np.random.seed(42)\n",
    "#tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = model.layers[1] # can also use: hidden1 = model.get_layer(\"dense\")\n",
    "print(\"Layer name:   \", hidden1.name)\n",
    "\n",
    "weights, biases = hidden1.get_weights()\n",
    "print(\"Weights shape:\", weights.shape)\n",
    "print(\"Biases shape: \", biases.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Second step: Compiling the model\n",
    "\n",
    "- After model creation, you need to call `compile()` to specify the loss function, optimiser and metrics to use.\n",
    "    * Loss function â€”This measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction.\n",
    "    * Optimiser â€”This is how the model is updated based on the data it sees and its loss function.\n",
    "    * Metrics â€”Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified. [Ref](https://www.tensorflow.org/tutorials/keras/classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])\n",
    "# can also use: \n",
    "# model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "#               optimizer=keras.optimizers.SGD(),\n",
    "#               metrics=[keras.metrics.sparse_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Third step: Training and evaluating the model\n",
    "<img src=\"https://raw.githubusercontent.com/rasbt/python-machine-learning-book-2nd-edition/master/code/ch06/images/06_02.png\" alt=\"Dataset splitting\" width=\"35%\" style=\"float:right; margin-left:3%;\"  />  \n",
    "- Finally, the model is ready. Simply call `fit()` to train (this is almost exactly the same as in sklearn). You need to specify the number of epochs, i.e. training cycles. \n",
    "- Specifying `validation_split` is not mandatory, but should usually be set to some small percentage. Keras will measure the loss and the extra metrics on a proportion (as specified) of the whole data set at the end of each epoch. **This helps massively in preventing overfitting.**  \n",
    "\n",
    "*Note:* Training may take a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=32, epochs=30, validation_split=0.1)\n",
    "# batch_size = 32 by default in the newer versions but for M1 mac, tf version is 2.0.0, \n",
    "# batch_size = 32 need to be put in, so just define this parameter to make things consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `fit()` function willl return a `History` object, which contains the training parameters, the list of epochs and most importantly, a dictionary `history.history`, which contains the loss and extra metrics at the end of each epoch for both the training and validation sets. \n",
    "- Using `pandas`, we can use the latter to plot the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Both training accuracy and validation accuracy went up during training, while both loss metrics decreased. This indicates a good model.\n",
    "- The validation curves are close to the training curves, which indicates that we do not have **overfitting**. \n",
    "- We can also see that the model has not quite yet converged. If you were to call the `fit()` function again, Keras would continue where it left off: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history2 = model.fit(X_train, y_train, epochs=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(history2.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To evaluate, simply use `evaluate()` on the test datasets. \n",
    "- It is common to get a slightly lower performance on the test set compared to training. \n",
    "- Just like before, predictions can be make with `predict()`, however this time the function will output the **prediction probabilities**. You should use `predict_classes()` to output single classes (the one with the highest probability). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model.evaluate(X_test, y_test, verbose = 2)\n",
    "print(\"Loss: \".ljust(12), evaluation[0])\n",
    "print(\"Accuracy: \".ljust(12), evaluation[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape/dimensions:\", X_train.shape)\n",
    "print(\"y_train:\", y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_subset_indices = np.random.randint(0,X_test.shape[0], size=(9,))\n",
    "\n",
    "X_new = X_test[random_subset_indices, :]\n",
    "y_proba = model.predict(X_new)\n",
    "print(\"Probabilities:\")\n",
    "print(y_proba.round(3), \"\\n\")\n",
    "\n",
    "#y_pred = model.predict_classes(X_new) this line would work in older versions of tf, use the line below for newer tf\n",
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "y_pred\n",
    "print(\"Class predictions:\", y_pred)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for index, image in enumerate(X_new):\n",
    "    plt.subplot(3, 3, index + 1)\n",
    "    plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\")\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Prediction: {}\\nTruth: {}\".format(\n",
    "        class_names[y_pred[index]], \n",
    "        class_names[y_test[random_subset_indices[index]]]))\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nav_menu": {
   "height": "264px",
   "width": "369px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
